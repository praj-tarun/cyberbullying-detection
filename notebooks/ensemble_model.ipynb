{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model for Cyberbullying Detection\n",
    "\n",
    "Combining LSTM and CNN predictions for improved cyberbullying detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from src.models import LSTMClassifier, SimpleCNN\n",
    "from src.utils import get_predictions, calculate_metrics\n",
    "from src.train import clean_text, build_vocab, TextDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "df[\"clean_text\"] = df[\"comment_text\"].apply(clean_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"clean_text\"].values, df[label_cols].values,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "vocab = build_vocab(X_train, max_vocab=10000)\n",
    "test_dataset = TextDataset(X_test, y_test, vocab, max_len=100)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lstm_model = LSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    output_dim=len(label_cols),\n",
    "    n_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "cnn_model = SimpleCNN(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=100,\n",
    "    n_filters=100,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    output_dim=len(label_cols),\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "try:\n",
    "    lstm_model.load_state_dict(torch.load('../outputs/lstm_model.pt', map_location=device))\n",
    "    cnn_model.load_state_dict(torch.load('../outputs/cnn_model.pt', map_location=device))\n",
    "    print(\"Models loaded successfully\")\n",
    "except:\n",
    "    print(\"Warning: Pre-trained models not found. Train LSTM and CNN models first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Individual Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities(model, iterator, device):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text = text.to(device)\n",
    "            predictions = model(text)\n",
    "            probs = torch.sigmoid(predictions)\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_probs), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_probs, y_true = get_probabilities(lstm_model, test_loader, device)\n",
    "cnn_probs, _ = get_probabilities(cnn_model, test_loader, device)\n",
    "\n",
    "print(f\"LSTM predictions shape: {lstm_probs.shape}\")\n",
    "print(f\"CNN predictions shape: {cnn_probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average ensemble\n",
    "ensemble_avg_probs = (lstm_probs + cnn_probs) / 2\n",
    "ensemble_avg_preds = (ensemble_avg_probs > 0.5).astype(float)\n",
    "\n",
    "# weighted ensemble (tune weights based on validation performance)\n",
    "lstm_weight = 0.6\n",
    "cnn_weight = 0.4\n",
    "ensemble_weighted_probs = lstm_weight * lstm_probs + cnn_weight * cnn_probs\n",
    "ensemble_weighted_preds = (ensemble_weighted_probs > 0.5).astype(float)\n",
    "\n",
    "# max ensemble\n",
    "ensemble_max_probs = np.maximum(lstm_probs, cnn_probs)\n",
    "ensemble_max_preds = (ensemble_max_probs > 0.5).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_preds = (lstm_probs > 0.5).astype(float)\n",
    "cnn_preds = (cnn_probs > 0.5).astype(float)\n",
    "\n",
    "metrics_lstm = calculate_metrics(y_true, lstm_preds, label_cols)\n",
    "metrics_cnn = calculate_metrics(y_true, cnn_preds, label_cols)\n",
    "metrics_avg = calculate_metrics(y_true, ensemble_avg_preds, label_cols)\n",
    "metrics_weighted = calculate_metrics(y_true, ensemble_weighted_preds, label_cols)\n",
    "metrics_max = calculate_metrics(y_true, ensemble_max_preds, label_cols)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"LSTM F1: {metrics_lstm['overall']['f1']:.4f}\")\n",
    "print(f\"CNN F1: {metrics_cnn['overall']['f1']:.4f}\")\n",
    "print(f\"Ensemble (Avg) F1: {metrics_avg['overall']['f1']:.4f}\")\n",
    "print(f\"Ensemble (Weighted) F1: {metrics_weighted['overall']['f1']:.4f}\")\n",
    "print(f\"Ensemble (Max) F1: {metrics_max['overall']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['LSTM', 'CNN', 'Ensemble\\n(Avg)', 'Ensemble\\n(Weighted)', 'Ensemble\\n(Max)']\n",
    "f1_scores = [\n",
    "    metrics_lstm['overall']['f1'],\n",
    "    metrics_cnn['overall']['f1'],\n",
    "    metrics_avg['overall']['f1'],\n",
    "    metrics_weighted['overall']['f1'],\n",
    "    metrics_max['overall']['f1']\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(models, f1_scores)\n",
    "plt.title('Model Comparison - Overall F1 Score')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.ylim([min(f1_scores) - 0.01, max(f1_scores) + 0.01])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/ensemble_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-label comparison\n",
    "lstm_f1 = [metrics_lstm[label]['f1'] for label in label_cols]\n",
    "cnn_f1 = [metrics_cnn[label]['f1'] for label in label_cols]\n",
    "ensemble_f1 = [metrics_avg[label]['f1'] for label in label_cols]\n",
    "\n",
    "x = np.arange(len(label_cols))\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(x - width, lstm_f1, width, label='LSTM')\n",
    "plt.bar(x, cnn_f1, width, label='CNN')\n",
    "plt.bar(x + width, ensemble_f1, width, label='Ensemble')\n",
    "\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Per-label F1 Score Comparison')\n",
    "plt.xticks(x, label_cols, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/ensemble_per_label.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ensemble = 'weighted'\n",
    "np.save('../outputs/ensemble_predictions.npy', ensemble_weighted_preds)\n",
    "print(f\"Best ensemble ({best_ensemble}) saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
