{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM vs CNN for Toxic Comment Classification\n",
    "\n",
    "Multi-label classification on Jigsaw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from src.models import LSTMClassifier, SimpleCNN\n",
    "from src.utils import train_epoch, evaluate, get_predictions, calculate_metrics\n",
    "from src.train import clean_text, build_vocab, TextDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 159571\n",
      "\n",
      "Label distribution:\n",
      "toxic            15294\n",
      "severe_toxic      1595\n",
      "obscene           8449\n",
      "threat             478\n",
      "insult            7877\n",
      "identity_hate     1405\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df[label_cols].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels per comment:\n",
      "num_labels\n",
      "0    143346\n",
      "1      6360\n",
      "2      3480\n",
      "3      4209\n",
      "4      1760\n",
      "5       385\n",
      "6        31\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check multi-label distribution\n",
    "df['num_labels'] = df[label_cols].sum(axis=1)\n",
    "print(\"Number of labels per comment:\")\n",
    "print(df['num_labels'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample comments:\n",
      "\n",
      "1. explanation why the edits made under my username hardcore metallica fan were reverted they werent va...\n",
      "   Labels: ['clean']\n",
      "\n",
      "2. daww he matches this background colour im seemingly stuck with thanks talk january utc...\n",
      "   Labels: ['clean']\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "df[\"clean_text\"] = df[\"comment_text\"].apply(clean_text)\n",
    "\n",
    "# check samples\n",
    "print(\"Sample comments:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\n{i+1}. {df['clean_text'].iloc[i][:100]}...\")\n",
    "    labels = [label_cols[j] for j in range(len(label_cols)) if df[label_cols[j]].iloc[i] == 1]\n",
    "    print(f\"   Labels: {labels if labels else ['clean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 127656, Test: 31915\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"clean_text\"].values, df[label_cols].values,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 10000\n"
     ]
    }
   ],
   "source": [
    "# build vocab\n",
    "vocab = build_vocab(X_train, max_vocab=10000)\n",
    "print(f\"Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, vocab, max_len=MAX_LEN)\n",
    "test_dataset = TextDataset(X_test, y_test, vocab, max_len=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "\n",
    "Bidirectional LSTM for sequence modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "Model params: 1632326\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "lstm_model = LSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    output_dim=len(label_cols),\n",
    "    n_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel params: {sum(p.numel() for p in lstm_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.087 Acc: 0.973 | Test Loss: 0.059 Acc: 0.980\n",
      "Epoch 02 | Train Loss: 0.058 Acc: 0.980 | Test Loss: 0.052 Acc: 0.982\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m---> 13\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate(lstm_model, test_loader, criterion, device)\n\u001b[0;32m     16\u001b[0m     lstm_train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[1;32mc:\\Users\\asus\\OneDrive\\Desktop\\IITJ\\DL\\notebooks\\..\\src\\utils.py:19\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, iterator, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels)\n\u001b[0;32m     17\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_multilabel(predictions, labels)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "lstm_train_losses = []\n",
    "lstm_test_losses = []\n",
    "lstm_train_accs = []\n",
    "lstm_test_accs = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(lstm_model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_acc = evaluate(lstm_model, test_loader, criterion, device)\n",
    "    \n",
    "    lstm_train_losses.append(train_loss)\n",
    "    lstm_test_losses.append(test_loss)\n",
    "    lstm_train_accs.append(train_acc)\n",
    "    lstm_test_accs.append(test_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1:02d} | Train Loss: {train_loss:.3f} Acc: {train_acc:.3f} | Test Loss: {test_loss:.3f} Acc: {test_acc:.3f}')\n",
    "\n",
    "lstm_train_time = time.time() - start_time\n",
    "print(f\"\\nTraining time: {lstm_train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "y_pred_lstm, y_true_lstm = get_predictions(lstm_model, test_loader, device)\n",
    "metrics_lstm = calculate_metrics(y_true_lstm, y_pred_lstm, label_cols)\n",
    "\n",
    "print(\"LSTM Overall Results:\")\n",
    "for k, v in metrics_lstm['overall'].items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPer-label F1 scores:\")\n",
    "for label in label_cols:\n",
    "    print(f\"{label}: {metrics_lstm[label]['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "\n",
    "Multi-filter CNN for local pattern detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = SimpleCNN(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=100,\n",
    "    n_filters=100,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    output_dim=len(label_cols),\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model params: {sum(p.numel() for p in cnn_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cnn = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "cnn_train_losses = []\n",
    "cnn_test_losses = []\n",
    "cnn_train_accs = []\n",
    "cnn_test_accs = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(cnn_model, train_loader, optimizer_cnn, criterion, device)\n",
    "    test_loss, test_acc = evaluate(cnn_model, test_loader, criterion, device)\n",
    "    \n",
    "    cnn_train_losses.append(train_loss)\n",
    "    cnn_test_losses.append(test_loss)\n",
    "    cnn_train_accs.append(train_acc)\n",
    "    cnn_test_accs.append(test_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1:02d} | Train Loss: {train_loss:.3f} Acc: {train_acc:.3f} | Test Loss: {test_loss:.3f} Acc: {test_acc:.3f}')\n",
    "\n",
    "cnn_train_time = time.time() - start_time\n",
    "print(f\"\\nTraining time: {cnn_train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "y_pred_cnn, y_true_cnn = get_predictions(cnn_model, test_loader, device)\n",
    "metrics_cnn = calculate_metrics(y_true_cnn, y_pred_cnn, label_cols)\n",
    "\n",
    "print(\"CNN Overall Results:\")\n",
    "for k, v in metrics_cnn['overall'].items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPer-label F1 scores:\")\n",
    "for label in label_cols:\n",
    "    print(f\"{label}: {metrics_cnn[label]['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0, 0].plot(lstm_train_losses, label='Train', alpha=0.7)\n",
    "axes[0, 0].plot(lstm_test_losses, label='Test', alpha=0.7)\n",
    "axes[0, 0].set_title('LSTM Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(lstm_train_accs, label='Train', alpha=0.7)\n",
    "axes[0, 1].plot(lstm_test_accs, label='Test', alpha=0.7)\n",
    "axes[0, 1].set_title('LSTM Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(cnn_train_losses, label='Train', alpha=0.7)\n",
    "axes[1, 0].plot(cnn_test_losses, label='Test', alpha=0.7)\n",
    "axes[1, 0].set_title('CNN Loss')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(cnn_train_accs, label='Train', alpha=0.7)\n",
    "axes[1, 1].plot(cnn_test_accs, label='Test', alpha=0.7)\n",
    "axes[1, 1].set_title('CNN Accuracy')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/lstm_cnn_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison table\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['LSTM', 'CNN'],\n",
    "    'Accuracy': [metrics_lstm['overall']['accuracy'], metrics_cnn['overall']['accuracy']],\n",
    "    'Precision': [metrics_lstm['overall']['precision'], metrics_cnn['overall']['precision']],\n",
    "    'Recall': [metrics_lstm['overall']['recall'], metrics_cnn['overall']['recall']],\n",
    "    'F1': [metrics_lstm['overall']['f1'], metrics_cnn['overall']['f1']],\n",
    "    'Train Time (s)': [lstm_train_time, cnn_train_time]\n",
    "})\n",
    "\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-label comparison\n",
    "per_label_f1 = pd.DataFrame({\n",
    "    'Label': label_cols,\n",
    "    'LSTM F1': [metrics_lstm[l]['f1'] for l in label_cols],\n",
    "    'CNN F1': [metrics_cnn[l]['f1'] for l in label_cols]\n",
    "})\n",
    "\n",
    "print(per_label_f1.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "torch.save(lstm_model.state_dict(), '../outputs/lstm_model.pt')\n",
    "torch.save(cnn_model.state_dict(), '../outputs/cnn_model.pt')\n",
    "\n",
    "with open('../outputs/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "with open('../outputs/label_cols.pkl', 'wb') as f:\n",
    "    pickle.dump(label_cols, f)\n",
    "\n",
    "print(\"Models saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "**LSTM observations:**\n",
    "- [Add your observations after running]\n",
    "\n",
    "**CNN observations:**\n",
    "- [Add your observations after running]\n",
    "\n",
    "**Multi-label challenges:**\n",
    "- Class imbalance (some labels are rare)\n",
    "- Label correlation (toxic often appears with insult)\n",
    "- Threshold selection matters\n",
    "\n",
    "**Next steps:**\n",
    "- Try ensemble methods\n",
    "- Test BERT\n",
    "- Experiment with class weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
