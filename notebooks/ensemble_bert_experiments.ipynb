{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Models: Ensemble and BERT\n",
    "\n",
    "Testing if combining models or using transformers helps with multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from src.models import LSTMClassifier, SimpleCNN\n",
    "from src.utils import get_predictions, calculate_metrics\n",
    "from src.train import clean_text, build_vocab, TextDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved artifacts\n",
    "with open('../outputs/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "with open('../outputs/label_cols.pkl', 'rb') as f:\n",
    "    label_cols = pickle.load(f)\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"Labels: {label_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data\n",
    "df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "df[\"clean_text\"] = df[\"comment_text\"].apply(clean_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"clean_text\"].values, df[label_cols].values,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(X_test, y_test, vocab, max_len=100)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lstm_model = LSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    output_dim=len(label_cols),\n",
    "    n_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "lstm_model.load_state_dict(torch.load('../outputs/lstm_model.pt', map_location=device))\n",
    "lstm_model.eval()\n",
    "\n",
    "cnn_model = SimpleCNN(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=100,\n",
    "    n_filters=100,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    output_dim=len(label_cols),\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "cnn_model.load_state_dict(torch.load('../outputs/cnn_model.pt', map_location=device))\n",
    "cnn_model.eval()\n",
    "\n",
    "print(\"Models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble: Averaging Predictions\n",
    "\n",
    "Average probabilities from LSTM and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "lstm_preds, y_true = get_predictions(lstm_model, test_loader, device)\n",
    "cnn_preds, _ = get_predictions(cnn_model, test_loader, device)\n",
    "\n",
    "print(f\"Predictions shape: {lstm_preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average ensemble\n",
    "ensemble_preds = (lstm_preds + cnn_preds) / 2\n",
    "\n",
    "metrics_ensemble = calculate_metrics(y_true, ensemble_preds, label_cols)\n",
    "\n",
    "print(\"Ensemble Results:\")\n",
    "for k, v in metrics_ensemble['overall'].items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check per-label improvement\n",
    "metrics_lstm = calculate_metrics(y_true, lstm_preds, label_cols)\n",
    "metrics_cnn = calculate_metrics(y_true, cnn_preds, label_cols)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Label': label_cols,\n",
    "    'LSTM F1': [metrics_lstm[l]['f1'] for l in label_cols],\n",
    "    'CNN F1': [metrics_cnn[l]['f1'] for l in label_cols],\n",
    "    'Ensemble F1': [metrics_ensemble[l]['f1'] for l in label_cols]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Fine-tuning\n",
    "\n",
    "Using pretrained transformer for multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "    from torch.utils.data import TensorDataset\n",
    "    BERT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"transformers not installed. Run: pip install transformers\")\n",
    "    BERT_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BERT_AVAILABLE:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # tokenize - using smaller subset for speed\n",
    "    train_size = 50000\n",
    "    X_train_bert = X_train[:train_size]\n",
    "    y_train_bert = y_train[:train_size]\n",
    "    \n",
    "    train_encodings = tokenizer(list(X_train_bert), truncation=True, padding=True, max_length=128)\n",
    "    test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "    train_dataset_bert = TensorDataset(\n",
    "        torch.tensor(train_encodings['input_ids']),\n",
    "        torch.tensor(train_encodings['attention_mask']),\n",
    "        torch.tensor(y_train_bert, dtype=torch.float)\n",
    "    )\n",
    "    test_dataset_bert = TensorDataset(\n",
    "        torch.tensor(test_encodings['input_ids']),\n",
    "        torch.tensor(test_encodings['attention_mask']),\n",
    "        torch.tensor(y_test, dtype=torch.float)\n",
    "    )\n",
    "    \n",
    "    train_loader_bert = DataLoader(train_dataset_bert, batch_size=16, shuffle=True)\n",
    "    test_loader_bert = DataLoader(test_dataset_bert, batch_size=16)\n",
    "    \n",
    "    print(f\"BERT datasets ready (using {train_size} training samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BERT_AVAILABLE:\n",
    "    bert_model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=len(label_cols),\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer_bert = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "    \n",
    "    print(f\"BERT params: {sum(p.numel() for p in bert_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BERT_AVAILABLE:\n",
    "    n_epochs_bert = 3\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs_bert):\n",
    "        bert_model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in train_loader_bert:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            optimizer_bert.zero_grad()\n",
    "            outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_bert.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader_bert)\n",
    "        print(f'Epoch {epoch+1}/{n_epochs_bert} | Train Loss: {avg_train_loss:.3f}')\n",
    "    \n",
    "    bert_train_time = time.time() - start_time\n",
    "    print(f\"\\nBERT training time: {bert_train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BERT_AVAILABLE:\n",
    "    bert_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_bert:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "            preds = (torch.sigmoid(outputs.logits) > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    bert_preds = np.array(all_preds)\n",
    "    bert_labels = np.array(all_labels)\n",
    "    \n",
    "    metrics_bert = calculate_metrics(bert_labels, bert_preds, label_cols)\n",
    "    \n",
    "    print(\"BERT Results:\")\n",
    "    for k, v in metrics_bert['overall'].items():\n",
    "        print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Model': ['LSTM', 'CNN', 'Ensemble'],\n",
    "    'F1': [metrics_lstm['overall']['f1'], metrics_cnn['overall']['f1'], metrics_ensemble['overall']['f1']]\n",
    "}\n",
    "\n",
    "if BERT_AVAILABLE:\n",
    "    results['Model'].append('BERT')\n",
    "    results['F1'].append(metrics_bert['overall']['f1'])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(results_df['Model'], results_df['F1'], alpha=0.7)\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Model Comparison - Multi-label Classification')\n",
    "plt.ylim([0.5, 1.0])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(results_df['F1']):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BERT_AVAILABLE:\n",
    "    torch.save(bert_model.state_dict(), '../outputs/bert_model.pt')\n",
    "    print(\"BERT model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Ensemble findings:**\n",
    "- [Add observations after running]\n",
    "\n",
    "**BERT findings:**\n",
    "- [Add observations after running]\n",
    "\n",
    "**Multi-label insights:**\n",
    "- Some labels are harder to predict (threat, identity_hate are rare)\n",
    "- Models might benefit from class weighting\n",
    "- Threshold tuning per label could help\n",
    "\n",
    "**Best model:**\n",
    "- [Note which performed best]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
